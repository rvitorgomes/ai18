{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rgomes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle as pickle\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementação da função de carregamento de textos \n",
    "def loadCorpus(sourcepath):\n",
    "    corpus ={}\n",
    "    for filename in os.listdir(sourcepath):\n",
    "        lines = []\n",
    "        with open(filename, 'rb') as f:\n",
    "            for line in f:\n",
    "                # transform unicode text to utf8 and ignore the erros\n",
    "                # the breaklines will be removed on processing\n",
    "                lines.append(line.decode(\"utf-8\", \"ignore\"))\n",
    "        corpus[filename] = lines\n",
    "    return corpus\n",
    "        \n",
    "def processCorpus(corpus , param_foldCase, param_language, param_stopWords,param_stemmer ):\n",
    "    newCorpus = {}\n",
    "    for document in corpus:\n",
    "        content = []\n",
    "        for sentence in corpus[document]:\n",
    "            sentence = sentence.rstrip('\\n')\n",
    "            sentence = foldCase(sentence,param_foldCase)\n",
    "            listOfTokens = tokenize(sentence)\n",
    "            listOfTokens = removeStopWords(listOfTokens,param_stopWords)\n",
    "            listOfTokens = applyStemming (listOfTokens,param_stemmer)\n",
    "            content.append(listOfTokens) #após os tratamentos acima a setença é inserida no array content \n",
    "        newCorpus[document] = content\n",
    "    return newCorpus \n",
    "\n",
    "def foldCase (line , parameter ):\n",
    "    if(parameter) : line  = line.lower()\n",
    "    return line\n",
    "\n",
    "def tokenize (line):\n",
    "    line = line.replace(\"_\", \" \")\n",
    "    regExpr = '\\W+'\n",
    "    return filter(None,re.split(regExpr,line))\n",
    "\n",
    "def removeStopWords(listOfTokens, listOfStopWords):\n",
    "    return [token for token in listOfTokens if token not in listOfStopWords]    \n",
    "\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamentoTexto (dicTexto):\n",
    "    corpus = {}\n",
    "    TextoAux = []\n",
    "    n = 0\n",
    "    contatorTextos = 0 \n",
    "    for texto in dicTexto:\n",
    "        for linha in dicTexto[texto]:\n",
    "            TextoAux.append (linha)\n",
    "            if (re.match(\"From:\",linha ) != None): #verifica se a linha começa com coma tag \"From:\"\n",
    "                n += 1\n",
    "                if (n == 2):\n",
    "                    contatorTextos +=1\n",
    "                    corpus[texto+str(contatorTextos)]= TextoAux.copy()\n",
    "                    TextoAux.clear()\n",
    "                    n=0\n",
    "        contatorTextos +=1\n",
    "        corpus[texto+str(contatorTextos)]= TextoAux.copy()\n",
    "        TextoAux.clear()\n",
    "        n=0\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d[1] = 'era uma casa muito engracada'\n",
    "    t2 = ['era', 'uma', 'casa', 'muito', 'engracada']\n",
    "    \n",
    "    d[2] = 'era uma vez'\n",
    "    t2 = ['era', 'uma', 'vez']\n",
    "    \n",
    "    # binary\n",
    "    features = t = ['era' 'uma' 'casa' 'muito' 'engracada' 'vez'] # todas as palavras\n",
    "    rowsBinary = \n",
    "    [\n",
    "        [1, 1, 1, 1, 0],\n",
    "        [1, 1, 0, 0, 1]\n",
    "    ] \n",
    "    \n",
    "    rowsTf = [\n",
    "        [1/5]\n",
    "    ]\n",
    "    \n",
    "    rowsTfIdf = [\n",
    "        [1/5]\n",
    "    ]\n",
    "    \n",
    "    # matriz\n",
    "    rows x features\n",
    "    \n",
    "    2000 x 82100\n",
    "    tf > 3\n",
    "    tfidf > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>>>>>>>>>>>>>>>>>>>>função que prepara o estágio de representação dos dados <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "def representCorpus(corpus):\n",
    "    newCorpus = {}\n",
    "    #neste ponto é gerado um único vetor de tokens relacionado a cada documento   \n",
    "    for document in corpus:\n",
    "        newCorpus[document] = [token for sentence in corpus[document] for token in sentence]\n",
    "\n",
    "    #cria uma lista com todos os tokens  que ocorrem em cada texto considerado.\n",
    "    allTokens  = []\n",
    "    for document in newCorpus:\n",
    "        allTokens = allTokens + list(set(newCorpus[document]))\n",
    "\n",
    "    #cria o dicionárido reverso, nesta etapa é contabilizado quantas vezes um determinado token aparecem no conjunto total de textos\n",
    "    idfDict = {}\n",
    "    for token in allTokens:\n",
    "        try :\n",
    "            idfDict[token] +=1\n",
    "        except KeyError:\n",
    "            idfDict[token] = 1 \n",
    "        \n",
    "   #armazena a quantidade de textos considerados\n",
    "    nDocument = len(corpus)\n",
    "    nToken = len(idfDict)    \n",
    "\n",
    " # atualiza o dicionario reverso, associando cada token com seu idf score\n",
    "    for token in idfDict:\n",
    "         idfDict[token] =log10(nDocument/float(idfDict[token]))    \n",
    "    \n",
    "     #computa a matriz termo-documento (newCorpus)\n",
    "    for document in newCorpus:\n",
    "        #computa um dicionario com os if scores de cada termo que ocorre no documento \n",
    "        dictOfTfScoredTokens = tf(newCorpus[document])\n",
    "\n",
    "        #computa um dicionário com o tf-idf score de cada par termo-documento\n",
    "        newCorpus[document] = ({token : dictOfTfScoredTokens[token] *idfDict[token] for token in dictOfTfScoredTokens })\n",
    "    \n",
    "    return newCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialise (obj,name):\n",
    "    f=open(name+ '.pkl', 'wb')\n",
    "    p = pickle.Pickler(f)\n",
    "    p.fast= True\n",
    "    p.dump(obj)\n",
    "    f.close()\n",
    "    p.clear_memo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(listOfTokens):\n",
    "    # cria um dicionario associando cada token com o numero de vezes\n",
    "    # em que ele ocorre no documento (cujo conteudo eh listOfTokens)\n",
    "    types = {}\n",
    "    for token in listOfTokens:\n",
    "        if (token in types.keys()) : types[token] += 1\n",
    "        else:                       types[token] = 1\n",
    "    return types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sourcepath = './texts'\n",
    "    filename = 'talk.religion.misc.txt'\n",
    "    #invocar a função de carregamento de textos\n",
    "    corpus1 = loadCorpus(sourcepath)\n",
    "\n",
    "    textosTratados = tratamentoTexto(corpus1) #neste passo é feito a delimitação dos textos que estão em cada arquivo txt\n",
    "\n",
    "\n",
    "    #definindo as variáveis para o pré-processamento\n",
    "    _foldCase = True\n",
    "    _language = 'english'\n",
    "    _stopWords = stopwords.words('english')\n",
    "    _stemmer = SnowballStemmer(_language)\n",
    "\n",
    "    #aqui ocorre o pré processamento ,Tokenização Remoção de stop words, case-folding , redução para o radical e lematização\n",
    "    corpus2 = processCorpus(textosTratados,_foldCase,_language,_stopWords,_stemmer)\n",
    "\n",
    "    #início da representação de dados \n",
    "    representCorpus(corpus2)\n",
    "\n",
    "    print(\"Process finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process finished\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf(all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
